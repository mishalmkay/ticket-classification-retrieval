{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# tickets_pipeline.py\n",
        "\n",
        "import pandas as pd\n",
        "import janitor\n",
        "import category_encoders as ce\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def main():\n",
        "    #  Load & basic cleaning\n",
        "    df = (\n",
        "        pd.read_csv(\"/content/dataset-tickets-multi-lang-4-20k.csv\", encoding=\"utf-8\")\n",
        "          .clean_names()\n",
        "          .remove_empty()\n",
        "          .drop_duplicates()\n",
        "    )\n",
        "\n",
        "    #  Drop all tag_* columns except tag_1\n",
        "    drop_tags = [c for c in df.columns if c.startswith(\"tag_\") and c != \"tag_1\"]\n",
        "    df.drop(columns=drop_tags, inplace=True)\n",
        "\n",
        "    #  Combine text fields\n",
        "    df[\"subject\"] = df[\"subject\"].fillna(\"\")\n",
        "    df[\"body\"]    = df[\"body\"].fillna(\"\")\n",
        "    df[\"text\"]    = (df[\"subject\"] + \" \" + df[\"body\"]).str.strip()\n",
        "    df = df[df[\"text\"] != \"\"]\n",
        "\n",
        "    #  Fill missing metadata with \"Unknown\"\n",
        "    for col in [\"queue\", \"priority\", \"language\", \"tag_1\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"Unknown\")\n",
        "\n",
        "    #  Merge very low‑frequency categories (<5%) into \"Other\"\n",
        "    def merge_low_freq(series, thresh=0.05):\n",
        "        freqs = series.value_counts(normalize=True)\n",
        "        rare  = freqs[freqs < thresh].index\n",
        "        return series.replace(rare, \"Other\")\n",
        "\n",
        "    for cat in [\"type\", \"queue\", \"priority\", \"language\", \"tag_1\"]:\n",
        "        if cat in df.columns:\n",
        "            df[cat] = merge_low_freq(df[cat])\n",
        "\n",
        "    #  Encode target labels → numeric\n",
        "    y_raw = df[\"type\"]\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "\n",
        "    #  Save mapping for later\n",
        "    print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "\n",
        "    X = df[[\"text\", \"queue\", \"priority\", \"language\", \"tag_1\"]]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        stratify=y,\n",
        "        test_size=0.20,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    #  Preprocessing: TF-IDF for text + TargetEncoder for metadata\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        (\"text_tfidf\",\n",
        "         TfidfVectorizer(strip_accents=\"unicode\", max_features=20_000),\n",
        "         \"text\"),\n",
        "        (\"meta_te\",\n",
        "         ce.TargetEncoder(cols=[\"queue\",\"priority\",\"language\",\"tag_1\"]),\n",
        "         [\"queue\",\"priority\",\"language\",\"tag_1\"]),\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    #  Full pipeline: preprocess → SMOTE → XGBoost\n",
        "    pipe = ImbPipeline(steps=[\n",
        "        (\"preproc\", preprocessor),\n",
        "        (\"smote\", SMOTE(random_state=42)),\n",
        "        (\"clf\", XGBClassifier(\n",
        "                    eval_metric=\"mlogloss\",\n",
        "                    use_label_encoder=False,\n",
        "                    random_state=42\n",
        "               )),\n",
        "    ])\n",
        "\n",
        "    #  Train\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    import joblib\n",
        "    joblib.dump(pipe, \"ticket_type_model.pkl\")\n",
        "    print(\" Model saved as ticket_type_model.pkl\")\n",
        "\n",
        "\n",
        "    y_pred_num = pipe.predict(X_test)\n",
        "    y_pred     = le.inverse_transform(y_pred_num)\n",
        "    y_true     = le.inverse_transform(y_test)\n",
        "    df[\"text\"]    = (df[\"subject\"] + \" \" + df[\"body\"]).str.strip()\n",
        "\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "DrIZw4YMvBLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from supabase import create_client, Client\n",
        "\n",
        "SUPABASE_KEY = \"\"\n",
        "SUPABASE_URL=\"\"\n",
        "TABLE_NAME = \"\"\n",
        "\n",
        "BATCH_SIZE = 100       # number of rows per insert\n",
        "BATCH_DELAY = 0.5      # delay between batches (seconds)\n",
        "CHUNK_MAX_LEN = 200    # max words per chunk\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "df = pd.read_csv(\"/content/dataset-tickets-multi-lang-4-20k.csv\")\n",
        "\n",
        "# Combine subject + body\n",
        "df[\"subject\"] = df[\"subject\"].fillna(\"\")\n",
        "df[\"body\"] = df[\"body\"].fillna(\"\")\n",
        "df[\"text\"] = df[\"subject\"] + \" \" + df[\"body\"]\n",
        "\n",
        "# If columns missing → default\n",
        "if \"resolution\" not in df.columns:\n",
        "    df[\"resolution\"] = \"No resolution available\"\n",
        "if \"type\" not in df.columns:\n",
        "    df[\"type\"] = \"Unknown\"\n",
        "\n",
        "print(f\" Loaded {len(df)} tickets\")\n",
        "\n",
        "def chunk_text(text, max_len=CHUNK_MAX_LEN):\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), max_len):\n",
        "        yield \" \".join(words[i:i+max_len])\n",
        "\n",
        "def insert_batch_safe(batch):\n",
        "    try:\n",
        "        supabase.table(TABLE_NAME).insert(batch).execute()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Insert failed, retrying in 2s...\", e)\n",
        "        time.sleep(2)\n",
        "        try:\n",
        "            supabase.table(TABLE_NAME).insert(batch).execute()\n",
        "            print(\"Retry success!\")\n",
        "            return True\n",
        "        except Exception as e2:\n",
        "            print(\"❌ Retry failed:\", e2)\n",
        "            return False\n",
        "\n",
        "batch_data = []\n",
        "total_inserted = 0\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    ticket_id = str(row.get(\"ticket_id\", uuid.uuid4()))\n",
        "    full_text = row[\"text\"]\n",
        "    resolution = row[\"resolution\"]\n",
        "    ticket_type = row[\"type\"]\n",
        "\n",
        "    # Split into chunks\n",
        "    for chunk_idx, chunk in enumerate(chunk_text(full_text)):\n",
        "        # Embed chunk\n",
        "        embedding = embed_model.encode(chunk).tolist()\n",
        "\n",
        "        metadata = {\n",
        "            \"ticket_id\": ticket_id,\n",
        "            \"ticket_type\": ticket_type,\n",
        "            \"resolution\": resolution,\n",
        "            \"chunk_idx\": chunk_idx\n",
        "        }\n",
        "\n",
        "        record = {\n",
        "            \"chunk_text\": chunk,\n",
        "            \"metadata\": metadata,\n",
        "            \"embedding\": embedding\n",
        "        }\n",
        "\n",
        "        batch_data.append(record)\n",
        "\n",
        "    if len(batch_data) >= BATCH_SIZE:\n",
        "        print(f\" Inserting batch of {len(batch_data)} rows...\")\n",
        "        success = insert_batch_safe(batch_data)\n",
        "        if success:\n",
        "            total_inserted += len(batch_data)\n",
        "        else:\n",
        "            print(\" Batch skipped due to repeated failure.\")\n",
        "        batch_data = []\n",
        "        time.sleep(BATCH_DELAY)\n",
        "\n",
        "if batch_data:\n",
        "    print(f\"🚀 Inserting final {len(batch_data)} rows...\")\n",
        "    success = insert_batch_safe(batch_data)\n",
        "    if success:\n",
        "        total_inserted += len(batch_data)\n",
        "\n",
        "print(f\"DONE! Total inserted rows: {total_inserted}\")\n"
      ],
      "metadata": {
        "id": "Jt0jjn4Pxyh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "from supabase import create_client, Client\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "\n",
        "SUPABASE_KEY = \"\"\n",
        "SUPABASE_URL=\"\"\n",
        "# TABLE_NAME = \"document_chunks\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "clf = joblib.load(\"ticket_type_model.pkl\")\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")  # must match DB dim\n",
        "\n",
        "label_mapping = {\n",
        "    0: \"Change\",\n",
        "    1: \"Incident\",\n",
        "    2: \"Problem\",\n",
        "    3: \"Request\"\n",
        "}\n",
        "\n",
        "EXPECTED_META = [\"queue\", \"priority\", \"language\", \"tag_1\"]\n",
        "\n",
        "\n",
        "def preprocess_ticket(subject: str, body: str, metadata: dict) -> dict:\n",
        "    \"\"\"Preprocess a single ticket to match training pipeline\"\"\"\n",
        "    subject = subject if subject else \"\"\n",
        "    body = body if body else \"\"\n",
        "    text = (subject + \" \" + body).strip()\n",
        "\n",
        "    # Fill missing metadata with 'Unknown'\n",
        "    clean_meta = {}\n",
        "    for key in EXPECTED_META:\n",
        "        clean_meta[key] = metadata.get(key, \"Unknown\") if metadata else \"Unknown\"\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"queue\": clean_meta[\"queue\"],\n",
        "        \"priority\": clean_meta[\"priority\"],\n",
        "        \"language\": clean_meta[\"language\"],\n",
        "        \"tag_1\": clean_meta[\"tag_1\"]\n",
        "    }\n",
        "\n",
        "def classify_ticket(ticket_features: dict) -> str:\n",
        "    \"\"\"\n",
        "    Predict ticket type using saved classifier.\n",
        "    Returns decoded label (Change/Incident/Problem/Request).\n",
        "    \"\"\"\n",
        "    # Must pass a DataFrame with correct column names\n",
        "    X = pd.DataFrame([{\n",
        "        \"text\": ticket_features[\"text\"],\n",
        "        \"queue\": ticket_features[\"queue\"],\n",
        "        \"priority\": ticket_features[\"priority\"],\n",
        "        \"language\": ticket_features[\"language\"],\n",
        "        \"tag_1\": ticket_features[\"tag_1\"]\n",
        "    }])\n",
        "\n",
        "    pred_num = clf.predict(X)[0]  # numeric prediction\n",
        "    return label_mapping.get(pred_num, \"Unknown\")\n",
        "\n",
        "def embed_text(text: str) -> list:\n",
        "    \"\"\"Embed text for Supabase RAG search\"\"\"\n",
        "    return embed_model.encode(text).tolist()\n",
        "\n",
        "def retrieve_similar_chunks(query_text: str, match_count: int = 3):\n",
        "    \"\"\"\n",
        "    Calls Supabase pgvector function `match_chunks()`\n",
        "    to retrieve top-k similar ticket chunks.\n",
        "    \"\"\"\n",
        "    query_embedding = embed_text(query_text)\n",
        "    response = supabase.rpc(\n",
        "        \"match_chunks\",\n",
        "        {\n",
        "            \"query_embedding\": query_embedding,\n",
        "            \"match_count\": match_count\n",
        "        }\n",
        "    ).execute()\n",
        "    return response.data\n",
        "\n",
        "def save_new_ticket_to_supabase(subject, body, metadata, predicted_type):\n",
        "    \"\"\"Embed and insert the new ticket into Supabase for future retrieval.\"\"\"\n",
        "    text = (subject + \" \" + body).strip()\n",
        "    embedding = embed_model.encode(text).tolist()\n",
        "\n",
        "    # Combine metadata + predicted type\n",
        "    full_metadata = {\n",
        "        \"predicted_type\": predicted_type,\n",
        "        **(metadata or {})  # include queue, priority, etc.\n",
        "    }\n",
        "\n",
        "    record = {\n",
        "        # let Supabase auto-generate 'id' if table has default uuid\n",
        "        \"chunk_text\": text,\n",
        "        \"metadata\": full_metadata,\n",
        "        \"embedding\": embedding\n",
        "    }\n",
        "\n",
        "    supabase.table(\"document_chunks\").insert(record).execute()\n",
        "    print(f\" New ticket saved in Supabase \")\n",
        "\n",
        "\n",
        "def classify_retrieve_save(subject: str, body: str, metadata: dict = None, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "    1. Preprocess ticket\n",
        "    2. Predict ticket type (decoded label)\n",
        "    3. Retrieve top-k similar historical chunks from Supabase\n",
        "    4. Save new ticket into Supabase for future retrieval\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Preprocess\n",
        "    ticket_features = preprocess_ticket(subject, body, metadata)\n",
        "    print(f\" Cleaned text: {ticket_features['text'][:100]}...\")\n",
        "\n",
        "    # Step 2: Classify\n",
        "    predicted_type = classify_ticket(ticket_features)\n",
        "    print(f\"Predicted Ticket Type: {predicted_type}\")\n",
        "\n",
        "    # Step 3: Retrieve similar chunks\n",
        "    similar_chunks = retrieve_similar_chunks(ticket_features[\"text\"], match_count=top_k)\n",
        "    print(\"\\n Top Similar Historical Tickets:\")\n",
        "    for match in similar_chunks:\n",
        "        print(f\"ID: {match['id']}\")\n",
        "        print(f\"Chunk: {match['chunk_text'][:100]}...\")\n",
        "        print(f\"Similarity: {round(match['similarity'], 4)}\\n\")\n",
        "\n",
        "    # Step 4: Save new ticket into Supabase\n",
        "    save_new_ticket_to_supabase(subject, body, metadata, predicted_type)\n",
        "\n",
        "    return {\n",
        "        \"predicted_type\": predicted_type,\n",
        "        \"similar_chunks\": similar_chunks\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    subject = \"Cannot login after OS update\"\n",
        "    body = \"After updating Windows, I cannot login to my laptop. It says wrong password even though it's correct.\"\n",
        "    metadata = {\n",
        "        \"queue\": \"Hardware\",\n",
        "        \"priority\": \"High\",\n",
        "        \"language\": \"English\",\n",
        "        \"tag_1\": \"Login Issue\"\n",
        "    }\n",
        "\n",
        "    result = classify_retrieve_save(subject, body, metadata, top_k=3)\n",
        "\n",
        "    print(\"\\n FINAL RESULT:\")\n",
        "    print(\"Predicted Ticket Type:\", result[\"predicted_type\"])\n",
        "    for match in result[\"similar_chunks\"]:\n",
        "        print(f\"- Similar Chunk: {match['chunk_text'][:80]}... (Score: {round(match['similarity'],4)})\")"
      ],
      "metadata": {
        "id": "ayl0S2EF1mn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMO8SZf1DjsQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}